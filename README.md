# ğŸ™ AI Voice Agent â€” 30 Days Challenge

An **end-to-end AI voice assistant** built using FastAPI, modern frontend technologies, and multiple AI APIs.  
The bot can **listen**, **understand**, **remember conversations**, and **speak back** â€” just like a human!

---
âœ¨ ## Features

ğŸ™ï¸ Voice Input â€“ Record speech and send it directly to the AI for processing.

ğŸ§  LLM Integration â€“ Uses Google Gemini to generate intelligent, context-aware responses.

ğŸ—£ï¸ Text-to-Speech Output â€“ Murf API converts LLM responses into natural-sounding speech.

ğŸ“ Speech-to-Text â€“ AssemblyAI transcribes user audio for the LLM to process.

ğŸ’¬ Conversation Memory â€“ Chat history stored by session to maintain context across interactions.

âš¡ Real-time Interaction â€“ Instant query-response loop for smooth voice conversations.

ğŸŒ Full-Stack Architecture â€“ FastAPI backend with HTML/CSS/JS frontend.

ğŸ› ï¸ Modular Design â€“ Clean separation of logic for easy scaling and feature addition.

## ğŸ“… Progress Timeline

| Day  | ğŸš€ Milestone | ğŸ“ Details |
|------|-------------|-----------|
| **Day 1** | ğŸ“‚ **Project Setup** | Initialized **FastAPI** backend + **HTML/CSS/JS** frontend. Set up environment variables & folder structure. |
| **Day 2** | ğŸŒ **Static Files & UI** | Served static assets via FastAPI. Built minimal UI to send requests to backend endpoints. |
| **Day 3** | ğŸ™ **Audio Upload Endpoint** | Created a file upload API to handle incoming audio files from the frontend. |
| **Day 4** | ğŸ› **Frontend Recording** | Implemented **MediaRecorder API** to record audio in browser, send as blob to backend. |
| **Day 5** | ğŸ—£ **Speech-to-Text (STT)** | Connected **AssemblyAI API** to transcribe uploaded audio. Tested full upload â†’ transcription pipeline. |
| **Day 6** | âš¡ **Optimized STT Flow** | Removed local file saves; enabled direct streaming to AssemblyAI for faster results. |
| **Day 7** | ğŸ”Š **Text-to-Speech (TTS)** | Integrated **Murf API** to convert AI text responses into realistic speech. |
| **Day 8** | ğŸ¤– **LLM Integration** | Added **Gemini API** to `/llm/query` for AI-powered text-based queries & responses. |
| **Day 9** | ğŸ¤ **Audio-to-Audio LLM** | Extended `/llm/query` to handle audio: Speech â†’ STT â†’ LLM â†’ TTS â†’ Playback. |
| **Day 10** | ğŸ’¬ **Chat History & Memory** | Implemented in-memory datastore with session IDs so conversations retain context. |
| **Day 11** | ğŸ”„ **Continuous Conversation** | Bot now automatically restarts recording after its response finishes playing. |
| **Day 12** | ğŸ¨ **UI & Performance Polish** | Upgraded frontend design, fixed autoplay bugs, and reduced response latency. |
| **Day 13** | ğŸ† **Refined UX & Session Control** | Improved natural flow, better session management, and smoother AI-human interaction. |

---

## ğŸ›  Technologies Used

### **Languages & Frameworks**
- ğŸ **Python 3.10+**
- âš¡ **FastAPI** (Backend)
- ğŸŒ **HTML5**, **CSS3**, **JavaScript (ES6)** (Frontend)

### **APIs & AI Services**
- ğŸ—£ **AssemblyAI** â€” Speech-to-Text (STT)
- ğŸ”Š **Murf AI** â€” Text-to-Speech (TTS)
- ğŸ¤– **Google Gemini API** â€” Large Language Model (LLM)

### **Tools & Libraries**
- ğŸ“¦ `requests` â€” API calls
- ğŸ¯ `pydantic` â€” Data validation
- ğŸ”„ `uvicorn` â€” ASGI server for FastAPI

---

## ğŸ— Architecture Overview

```plaintext
ğŸ¤ User Speaks
    â†“
ğŸ™ Browser Records Audio (MediaRecorder API)
    â†“
ğŸ“¡ Send to FastAPI Endpoint (/agent/chat/{session_id})
    â†“
ğŸ—£ AssemblyAI â†’ Transcribe Audio
    â†“
ğŸ¤– Gemini API â†’ Generate AI Response
    â†“
ğŸ”Š Murf API â†’ Convert Text to Speech
    â†“
ğŸ§ Browser Plays AI Response
    â†“
ğŸ” Continue Conversation with Memory
